{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdad9e2-c0ba-43ac-96fc-f65b6bd2bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a09f36-0400-48aa-9b4b-c50e5a82084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extracting general details\n",
    "    general_details = {}\n",
    "    for i, line in enumerate(lines):\n",
    "        if i < 9:\n",
    "            if line.strip():\n",
    "                key, value = line.split(maxsplit=1)\n",
    "                general_details[key.strip()] = value.strip()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Extracting hits table\n",
    "    hits = []\n",
    "    for line in lines[9:]:\n",
    "        if not line.startswith(\"No\"):\n",
    "            # hit_info = line[:30].strip() + \" \" + line[30:].strip()\n",
    "            if line.strip():\n",
    "                hits.append(line)\n",
    "        elif line.startswith(\"No\"):\n",
    "            break\n",
    "\n",
    "    # Extracting full hit names\n",
    "    full_hits = {}\n",
    "    hit_details = \"\"\n",
    "    hit_id = None\n",
    "    for line in lines[9 + len(hits) + 1:]:\n",
    "        if line.startswith(\"No\"):\n",
    "            hit_id = int(line.split()[1])\n",
    "        elif line.startswith(\">\"):\n",
    "            full_hits[hit_id] = line.strip()[1:]\n",
    "    return general_details, hits, full_hits\n",
    "\n",
    "def extract_non_overlapping_hits(hits, full_hits):\n",
    "    def parse_coords(coords):\n",
    "        start, end = coords.split('-')\n",
    "        return int(start), int(end)\n",
    "\n",
    "    non_overlapping_hits = []\n",
    "    occupied_regions = []\n",
    "\n",
    "    for hit in hits:\n",
    "        # Assuming $line is the input string\n",
    "        hit_parts = [hit[:3], hit[3:34], hit[34:40], hit[40:48], hit[48:56], hit[56:63], hit[63:69], hit[69:74], hit[74:84], hit[84:93], hit[93:]]\n",
    "        hit_parts = [s.strip() for s in hit_parts]\n",
    "        # print (hit_parts)\n",
    "\n",
    "        # hit_parts = hit.split(\" \",1)\n",
    "        hit_id = int(hit_parts[0])\n",
    "        query_coords = hit_parts[8]\n",
    "        query_start, query_end = parse_coords(query_coords)\n",
    "\n",
    "        overlap = False\n",
    "        for start, end in occupied_regions:\n",
    "            if not (query_end < start + 5 or query_start > end - 5):\n",
    "                overlap = True\n",
    "                break\n",
    "\n",
    "        if not overlap:\n",
    "            non_overlapping_hits.append({\n",
    "                'No': hit_id,\n",
    "                'Hit': hit_parts[1],\n",
    "                'Prob': hit_parts[2],\n",
    "                'Eval': hit_parts[3],\n",
    "                'Cols': hit_parts[7],\n",
    "                'Qf': query_start,\n",
    "                'Qt': query_end,\n",
    "                'Tf': hit_parts[9].split(\"-\")[0],\n",
    "                'Tt': hit_parts[9].split(\"-\")[1],\n",
    "                'FullName': full_hits[hit_id]\n",
    "                \n",
    "            })\n",
    "            occupied_regions.append((query_start, query_end))\n",
    "\n",
    "    return non_overlapping_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8bf09a-a834-4094-97aa-e1ad1c1b3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on all proteins in the folder, create one table\n",
    "\n",
    "# List to accumulate all non-overlapping hits for all proteins\n",
    "all_hits = []\n",
    "\n",
    "# Iterate over all files\n",
    "for file_path in glob.iglob(f'/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/hhsrch/*'): \n",
    "    base_name = os.path.basename(file_path)\n",
    "    protein_name = os.path.splitext(base_name)[0]\n",
    "    protein_name = os.path.splitext(protein_name)[0] # needed a second to remove additional suffix\n",
    "\n",
    "    # Parse the file and extract hits\n",
    "    general_details, hits, full_hits = parse_file(file_path)\n",
    "    non_overlapping_hits = extract_non_overlapping_hits(hits, full_hits)\n",
    "\n",
    "    # Plot system for visualization (if needed)\n",
    "#    plot_system(non_overlapping_hits, int(general_details['Match_columns']))\n",
    "\n",
    "    # Add protein name to each hit and accumulate all non-overlapping hits\n",
    "    for hit in non_overlapping_hits:\n",
    "        hit['Protein'] = protein_name  # Add protein name to the hit data\n",
    "        all_hits.append(hit)\n",
    "\n",
    "    # Save plot to seperate files\n",
    "#    plt.savefig(f'../hhpred/IS_W0001_phage_diverse/parsed/figs/{protein_name}.svg', dpi=300, bbox_inches='tight')\n",
    "#    plt.close()\n",
    "\n",
    "\n",
    "# Convert the list of all hits to a DataFrame\n",
    "keys_order = ['Protein', 'No', 'Hit', 'Prob', 'Eval', 'Cols', 'Qf', 'Qt', 'Tf', 'Tt', 'FullName']\n",
    "df = pd.DataFrame(all_hits, columns=keys_order)\n",
    "\n",
    "# Write the DataFrame to a single tab-delimited file\n",
    "output_file_path = '~/hhpred/neighbors/mEp506_HicABC_system/parsed/mEp506_HicABC_system_neighbor_hits_summary.tsv' \n",
    "df.to_csv(output_file_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1bf61-00c3-4e14-822d-80a1c09bbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HicABC_phrogs= pd.read_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/parsed/mEp506_HicABC_system_neighbor_hits_summary.tsv',usecols=[0,1,2,3,4,5,6,7,8,9,10],names=['Protein', 'No', 'Hit', 'Prob', 'Eval', 'Cols', 'Qf', 'Qt', 'Tf', 'Tt', 'FullName'],sep='\\t',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c37a73-5ee5-4d3a-b404-8442791b1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "HicABC_phrogs_sorted=HicABC_phrogs.sort_values(by='Prob', ascending=False,ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a704c6-e26d-432d-a1af-0bd6e6c85efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HicABC_phrogs_sorted_nodups=HicABC_phrogs_sorted.drop_duplicates(subset=\"Protein\",keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675a374-b9ad-40d1-97fd-52a83f45b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HicABC_phrogs_sorted_nodups.to_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/parsed/mEp506_HicABC_phrogs_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429baca1-0968-482f-9b5b-803f646125e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HicABC_phrogs_NB= pd.read_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/parsed/mEp506_HicABC_phrogs_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da581c-ee2c-4613-bf6a-1e84a825c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HicABC_neighbors_FTs= pd.read_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/mEp506_HicABC_neighbors_detailed_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13717fa-3208-49fc-8fdd-8a1ef2b33216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "HicABC_phrogs_NB = pd.read_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/parsed/mEp506_HicABC_phrogs_cleaned.csv')\n",
    "HicABC_neighbors_FTs = pd.read_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/mEp506_HicABC_neighbors_detailed_summary.csv')\n",
    "\n",
    "# Merge the DataFrames on the matching columns\n",
    "merged_df = pd.merge(\n",
    "    HicABC_neighbors_FTs,\n",
    "    HicABC_phrogs_NB[['Protein','Prob', 'Eval', 'Cols','FullName']],  # Select only the necessary columns\n",
    "    left_on='non-redundant_refseq',  # Column name in DUF_phrogs_NB\n",
    "    right_on='Protein',  # Column name in PhiR5_1_neighbors_FTs\n",
    "    how='left'  # Use 'left' join to keep all rows from DUF_phrogs_NB\n",
    ")\n",
    "\n",
    "# Drop the 'non-redundant refseq' column if not needed in the final DataFrame\n",
    "merged_df.drop(columns=['Protein'], inplace=True)\n",
    "\n",
    "merged_df.to_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/parsed/mEp506_HicABC_phrogs_NB.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "### slight difference in hit #s are because non-redundant_refseq WPs that have been \"suppressed\" were not gathered by batch entrez for fasta info.\n",
    "# it looks to be rather minor of a percentage tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a0d78-3d3c-4e42-8a04-75233a6b350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "mEp506_HicABC_phrogs_NB = pd.read_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/parsed/mEp506_HicABC_phrogs_NB.csv')\n",
    "\n",
    "def check_prophage(prob_series):\n",
    "    total_count = len(prob_series)\n",
    "    high_prob_count = (prob_series <= 0.0000000001).sum()\n",
    "    if high_prob_count / total_count >= 0.15:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Calculate the count of entries for each assembly\n",
    "assembly_counts = mEp506_HicABC_phrogs_NB['assembly'].value_counts()\n",
    "\n",
    "# Filter assemblies with at least 10 entries\n",
    "valid_assemblies = assembly_counts[assembly_counts >= 10].index\n",
    "\n",
    "# Filter the original DataFrame to include only valid assemblies\n",
    "filtered_df = mEp506_HicABC_phrogs_NB[mEp506_HicABC_phrogs_NB['assembly'].isin(valid_assemblies)]\n",
    "\n",
    "# Group by 'assembly' and apply the function to the 'Prob' column\n",
    "result_df = filtered_df.groupby('assembly')['Eval'].apply(check_prophage).reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "result_df.rename(columns={'Prob': 'prophage'}, inplace=True)\n",
    "\n",
    "# Save the resulting DataFrame\n",
    "\n",
    "result_df.to_csv('/home/gridsan/lbrenes/hhpred/neighbors/mEp506_HicABC_system/parsed/mEp506_HicABC_prophage_hits.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
